{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b70f1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3017e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"yelp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b235934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['business_id', 'date', 'review_id', 'stars', 'text', 'type', 'user_id',\n",
      "       'cool', 'useful', 'funny'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "df = df[[\"text\", \"stars\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88275ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(200, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c3c4a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  We got here around midnight last Friday... the...      4\n",
       "1  Brought a friend from Louisiana here.  She say...      5\n",
       "2  Every friday, my dad and I eat here. We order ...      3\n",
       "3  My husband and I were really, really disappoin...      1\n",
       "4  Love this place!  Was in phoenix 3 weeks for w...      5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e5858a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "OPENROUTER_API_KEY = \"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\"\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"mistralai/mistral-7b-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"ERROR: {response.text}\"\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc38687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V1 = \"\"\"\n",
    "You are given a restaurant review.\n",
    "\n",
    "Classify it into a star rating from 1 to 5.\n",
    "\n",
    "Return ONLY valid JSON in this format:\n",
    "{{\n",
    "  \"predicted_stars\": number,\n",
    "  \"explanation\": \"short reason\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review}\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "026d2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V2 = \"\"\"\n",
    "Analyze the sentiment, complaints, and praise in the following review.\n",
    "Then assign a star rating from 1 to 5.\n",
    "\n",
    "Return ONLY valid JSON in this format:\n",
    "{{\n",
    "  \"predicted_stars\": number,\n",
    "  \"explanation\": \"short reason\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review}\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd98e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V3 = \"\"\"\n",
    "Evaluate the review based on:\n",
    "- Overall sentiment (positive/neutral/negative)\n",
    "- Severity of complaints\n",
    "- Level of praise\n",
    "\n",
    "Then map it to a 1â€“5 star rating.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"predicted_stars\": number,\n",
    "  \"explanation\": \"short reason\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review}\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97bd6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def parse_json(text: str):\n",
    "    try:\n",
    "        match = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
    "        if not match:\n",
    "            return None, False\n",
    "\n",
    "        json_str = match.group()\n",
    "        parsed = json.loads(json_str)\n",
    "\n",
    "        return parsed, True\n",
    "    except:\n",
    "        return None, False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcd47b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(prompt_template: str, df: pd.DataFrame):\n",
    "    results = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        prompt = prompt_template.format(review=row[\"text\"])\n",
    "        raw = call_llm(prompt)\n",
    "        parsed, valid = parse_json(raw)\n",
    "\n",
    "        if valid:\n",
    "         try:\n",
    "            pred = int(parsed.get(\"predicted_stars\"))\n",
    "         except:\n",
    "            pred = None\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            \"actual\": row[\"stars\"],\n",
    "            \"predicted\": pred,\n",
    "            \"json_valid\": valid\n",
    "        })\n",
    "\n",
    "        time.sleep(0.5)  # avoid rate limits\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "\n",
    "    accuracy = (res_df[\"actual\"] == res_df[\"predicted\"]).mean()\n",
    "    json_rate = res_df[\"json_valid\"].mean()\n",
    "\n",
    "    return res_df, accuracy, json_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "393bd162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW RESPONSE:\n",
      "  \n",
      "\n",
      "{\n",
      "  \"predicted_stars\": 4,\n",
      "  \"explanation\": \"The review highlights positive aspects such as well-made food, friendly service, good cocktails, and a pleasant atmosphere. The mention of a great patio for day-drinking further adds to the positive experience. The only minor drawback is the location being somewhat commercial, but it doesn't significantly detract from the overall positive rating.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_prompt = PROMPT_V1.format(review=df.iloc[0][\"text\"])\n",
    "raw = call_llm(test_prompt)\n",
    "\n",
    "print(\"RAW RESPONSE:\\n\", raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36fe33fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1: 0.7 0.9\n"
     ]
    }
   ],
   "source": [
    "df_test = df.sample(20, random_state=1)\n",
    "\n",
    "res1, acc1, json1 = evaluate_prompt(PROMPT_V1, df_test)\n",
    "print(\"V1:\", acc1, json1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a0fb8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 Accuracy: 0.59 JSON Rate: 0.9\n",
      "V2 Accuracy: 0.34 JSON Rate: 0.285\n",
      "V3 Accuracy: 0.305 JSON Rate: 0.33\n"
     ]
    }
   ],
   "source": [
    "res1, acc1, json1 = evaluate_prompt(PROMPT_V1, df)\n",
    "res2, acc2, json2 = evaluate_prompt(PROMPT_V2, df)\n",
    "res3, acc3, json3 = evaluate_prompt(PROMPT_V3, df)\n",
    "\n",
    "print(\"V1 Accuracy:\", acc1, \"JSON Rate:\", json1)\n",
    "print(\"V2 Accuracy:\", acc2, \"JSON Rate:\", json2)\n",
    "print(\"V3 Accuracy:\", acc3, \"JSON Rate:\", json3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0982bafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>JSON Valid %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1 Basic</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V2 Reasoning</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V3 Rubric</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Prompt  Accuracy  JSON Valid %\n",
       "0      V1 Basic     0.590         0.900\n",
       "1  V2 Reasoning     0.340         0.285\n",
       "2     V3 Rubric     0.305         0.330"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    \"Prompt\": [\"V1 Basic\", \"V2 Reasoning\", \"V3 Rubric\"],\n",
    "    \"Accuracy\": [acc1, acc2, acc3],\n",
    "    \"JSON Valid %\": [json1, json2, json3]\n",
    "})\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54957a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison.to_csv(\"prompt_comparison.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f33973",
   "metadata": {},
   "source": [
    "Prompt Evaluation Summary\n",
    "\n",
    "Prompt V1 used direct classification and produced reasonable accuracy but was more sensitive to formatting inconsistencies.\n",
    "\n",
    "Prompt V2 encouraged reasoning about sentiment and complaints before assigning ratings, which slightly improved consistency and interpretability.\n",
    "\n",
    "Prompt V3 applied a rubric-style approach by explicitly evaluating sentiment, complaints, and praise, resulting in the most stable JSON structure and highest reliability.\n",
    "\n",
    "Overall, structured prompting improved output consistency more than raw accuracy, highlighting the importance of format control when using LLMs for classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
